{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser import parse_pdf_to_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLoRA = \"https://arxiv.org/pdf/2305.14314\"\n",
    "DistillBERT = \"https://arxiv.org/pdf/1910.01108\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing temp.pdf...\n",
      "[                                        ] (0/5=======[========                                ] (1/5=======[================                        ] (2/5=======[========================                ] (3/5=======[================================        ] (4/5=======[========================================] (5/5]\n"
     ]
    }
   ],
   "source": [
    "markdown = parse_pdf_to_markdown(DistillBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"QLoRA.md\", \"w\") as f:\n",
    "    f.write(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter'}, page_content='**Victor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF**\\nHugging Face\\n```\\n{victor,lysandre,julien,thomas}@huggingface.co\\n\\n```'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': 'Abstract'}, page_content='As Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-specific\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\\ncapabilities for on-device computations in a proof-of-concept experiment and a\\ncomparative on-device study.'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': '1 Introduction'}, page_content='The last two years have seen the rise\\nof Transfer Learning approaches in\\nNatural Language Processing (NLP)\\nwith large-scale pre-trained language\\nmodels becoming a basic tool in\\nmany NLP tasks [Devlin et al., 2018,\\nRadford et al., 2019, Liu et al., 2019].\\nWhile these models lead to significant improvement, they often have\\nseveral hundred million parameters\\nand current research[1] on pre-trained\\nmodels indicates that training even\\nlarger models still leads to better performances on downstream tasks.  \\nFigure 1: Parameter counts of several recently released  \\nThe trend toward bigger models  \\n**pretrained language models.**  \\nraises several concerns. First is the\\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\\nin real-time has the potential to enable novel and interesting language processing applications, the\\ngrowing computational and memory requirements of these models may hamper wide adoption.  \\n[1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)](https://nv-adlr.github.io/MegatronLM)  \\nEMC^2: 5th Edition Co-located with NeurIPS’19  \\n-----  \\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks\\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\\nthat are lighter and faster at inference time, while also requiring a smaller computational training\\nbudget. Our general-purpose pre-trained models can be fine-tuned with good performances on several\\ndownstream tasks, keeping the flexibility of larger models. We also show that our compressed models\\nare small enough to run on the edge, e.g. on mobile devices.  \\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\\nablation studies indicate that all the components of the triple loss are important for best performances.  \\nWe have made the trained weights available along with the training code in the Transformers[2]\\nlibrary from HuggingFace [Wolf et al., 2019].'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': '2 Knowledge distillation'}, page_content='**Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which**\\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher or an ensemble of models.  \\nIn supervised learning, a classification model is generally trained to predict an instance class by\\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\\ndistribution of training labels. A model performing well on the training set will predict an output\\ndistribution with high probability on the correct class and with near-zero probabilities on other\\nclasses. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the\\ngeneralization capabilities of the model and how well it will perform on the test set[3].  \\n**Training loss The student is trained with a distillation loss over the soft target probabilities of**\\nthe teacher: Lce = [�]i _[t][i][ ∗]_ [log(][s][i][)][ where][ t][i][ (resp.][ s][i][) is a probability estimated by the teacher]  \\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature: pi = �exp(j [exp(]zi[z]/T[j] _[/T] )_ [ )]  \\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i.\\nThe same temperature T is applied to the student and the teacher at training time, while at inference,\\n_T is set to 1 to recover a standard softmax._  \\nThe final training objective is a linear combination of the distillation loss Lce with the supervised\\ntraining loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it\\nbeneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\\nand teacher hidden states vectors.'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': '3 DistilBERT: a distilled version of BERT'}, page_content='**Student architecture In the present work, the student - DistilBERT - has the same general architec-**\\nture as BERT. The token-type embeddings and the pooler are removed while the number of layers\\nis reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear\\n_layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our_\\ninvestigations showed that variations on the last dimension of the tensor (hidden size dimension) have\\na smaller impact on computation efficiency (for a fixed parameters budget) than variations on other\\nfactors like the number of layers. Thus we focus on reducing the number of layers.  \\n**Student initialization In addition to the previously described optimization and architectural choices,**\\nan important element in our training procedure is to find the right initialization for the sub-network to\\nconverge. Taking advantage of the common dimensionality between teacher and student networks,\\nwe initialize the student from the teacher by taking one layer out of two.  \\n2https://github.com/huggingface/transformers\\n3E.g. BERT-base’s predictions for a masked token in \"I think this is the beginning of a\\n```\\nbeautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions\\n\\n```\\n(future, story, world...).  \\n-----  \\nTable 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\\nmedians of 5 runs with different seeds.  \\nModel **Score** CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI  \\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3  \\nTable 2: DistilBERT yields to comparable\\n**performance on downstream tasks. Com-**\\nparison on downstream tasks: IMDb (test accuracy) and SQuAD 1.1 (EM/F1 on dev set).\\nD: with a second step of distillation during\\nfine-tuning.  \\nModel IMDb SQuAD\\n(acc.) (EM/F1)  \\nBERT-base 93.46 81.2/88.5\\nDistilBERT 92.82 77.7/85.8\\nDistilBERT (D)  - 79.1/86.9  \\nTable 3: DistilBERT is significantly smaller\\n**while being constantly faster.** Inference\\ntime of a full pass of GLUE task STS-B (sentiment analysis) on CPU with a batch size of\\n1.  \\nModel # param. Inf. time\\n(Millions) (seconds)  \\nELMo 180 895\\nBERT-base 110 668\\nDistilBERT 66 410  \\n**Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].**\\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\\nexamples per batch) using dynamic masking and without the next sentence prediction objective.  \\n**Data and compute power We train DistilBERT on the same corpus as the original BERT model:**\\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': '4 Experiments'}, page_content='**General Language Understanding We assess the language understanding and generalization ca-**\\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark  \\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\\nWe report scores on the development sets for each task by fine-tuning DistilBERT without the use\\nof ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present\\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\\net al. [2018]) encoder followed by two BiLSTMs.[4]  \\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\\nBERT, retaining 97% of the performance with 40% fewer parameters.  \\n**4.1** **Downstream task benchmark**  \\n**Downstream tasks We further study the performances of DistilBERT on several downstream tasks**\\nunder efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al.  \\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).  \\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.  \\nWe also studied whether we could add another step of distillation during the adaptation phase by\\nfine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a  \\n4We use jiant [Wang et al., 2019] to compute the baseline.  \\n-----  \\nTable 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\\nweights initialization.  \\nAblation **Variation on GLUE macro-score**  \\n_∅_     - Lcos - Lmlm -2.96\\n_Lce - ∅_      - Lmlm -1.46\\n_Lce - Lcos - ∅_ -0.31\\nTriple loss + random weights initialization -3.69  \\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\\n70.4 EM, i.e. within 3 points of the full model.  \\n**Size and inference speed**  \\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\\nof parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.  \\n**On device computation We studied whether DistilBERT could be used for on-the-edge applications**\\nby building a mobile application for question answering. We compare the average inference time on\\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available[5].  \\n**4.2** **Ablation study**  \\nIn this section, we investigate the influence of various components of the triple loss and the student\\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\\nimpact while the two distillation losses account for a large portion of the performance.'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': '5 Related work'}, page_content='**Task-specific distillation Most of the prior works focus on building task-specific distillation se-**\\ntups. Tang et al. [2019] transfer fine-tune classification model BERT to an LSTM-based classifier.\\nChatterjee [2019] distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose\\npre-training distillation rather than a task-specific distillation. Turc et al. [2019] use the original\\npretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher’s knowledge to pre-train with additional\\ndistillation signal.  \\n**Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using**\\nmulti-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation\\nto learn a compact question answering model from a set of large question answering models. An\\napplication of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us\\nby pre-training a multilingual model from scratch solely through distillation. However, as shown in\\nthe ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads\\nto substantial gains.  \\n**Other compression techniques have been studied to compress large models. Recent developments**\\nin weights pruning reveal that it is possible to remove some heads in the self-attention at test time\\nwithout significantly degrading the performance Michel et al. [2019]. Some layers can be reduced\\nto one head. A separate line of study leverages quantization to derive smaller models (Gupta et al.  \\n[2015]). Pruning and quantization are orthogonal to the present work.  \\n5https://github.com/huggingface/swift-coreml-transformers  \\n-----'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': '6 Conclusion and future work'}, page_content='We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\\nlanguage model can be successfully trained with distillation and analyzed the various components\\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\\napplications.'),\n",
       " Document(metadata={'Header 2': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'Header 3': 'References'}, page_content='Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. In NAACL-HLT, 2018.  \\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\\nunsupervised multitask learners. 2019.  \\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv,\\nabs/1907.11692, 2019.  \\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv, abs/1907.10597, 2019.  \\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\\nnlp. In ACL, 2019.  \\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. In NIPS, 2017.  \\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\\nprocessing, 2019.  \\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006.  \\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv,\\nabs/1503.02531, 2015.  \\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV), pages 19–27, 2015.  \\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR, 2018.  \\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. Deep contextualized word representations. In NAACL, 2018.  \\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\\n[text understanding models. http://jiant.info/, 2019.](http://jiant.info/)  \\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning\\nword vectors for sentiment analysis. In ACL, 2011.  \\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\\ncomprehension of text. In EMNLP, 2016.  \\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific\\nknowledge from bert into simple neural networks. ArXiv, abs/1903.12136, 2019.  \\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv, abs/1904.00796, 2019.  \\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\\nof student initialization on knowledge distillation. ArXiv, abs/1908.08962, 2019.  \\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\\ndistillation for web-scale question answering system. ArXiv, abs/1904.09636, 2019.  \\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practical\\nbert models for sequence labeling. In EMNLP-IJCNLP, 2019.  \\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019.  \\nSuyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited\\nnumerical precision. In ICML, 2015.  \\n-----')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown)\n",
    "md_header_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
